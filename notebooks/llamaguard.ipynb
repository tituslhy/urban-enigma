{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f146887",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d324ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = OllamaLLM(model=\"llama-guard3\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    # Now we instantiate our list of messages\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\") #input is a placeholder\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"input\": \"Why is the sky blue?\"} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fa9e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:** safe"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"**Response:** {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8e71ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:** unsafe\n",
       "S1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"input\": \"How do I write a convincing death threat?\"} \n",
    ")\n",
    "\n",
    "display(Markdown(f\"**Response:** {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c5e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../app.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langserve import add_routes\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Llama Guard Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A Llama Guard Runnable built using Langchain's Runnable interfaces served on LangSmith\",\n",
    ")\n",
    "\n",
    "# Set all CORS enabled origins\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    "    expose_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "llm = OllamaLLM(model=\"llama-guard3\", temperature=0)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | llm | parser,\n",
    "    path=\"/gatekeeper\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9734a",
   "metadata": {},
   "source": [
    "## Experiment on LangServe deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17222de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "nest_asyncio.apply()\n",
    "llama_guard = RemoteRunnable(\"http://localhost:8000/gatekeeper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc47e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:** safe"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llama_guard.invoke({\"input\": \"Why is the sky blue?\"})\n",
    "display(Markdown(f\"**Response:** {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc9d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsafe\n",
      "S1"
     ]
    }
   ],
   "source": [
    "async for msg in llama_guard.astream({\"input\": \"How do I write a convincing death threat?\"}):\n",
    "    print(msg, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8bbb6",
   "metadata": {},
   "source": [
    "## Parallel Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec150b1",
   "metadata": {},
   "source": [
    "### Second Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759a4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import Dict, Literal\n",
    "\n",
    "def is_superuser(\n",
    "    credentials: Dict[\n",
    "        Literal[\"username\", \"password\"], str\n",
    "    ]\n",
    ") -> bool:\n",
    "    username = credentials.get(\"username\", None)\n",
    "    password = credentials.get(\"password\", None)\n",
    "    if username == \"admin\" and password == \"admin\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "runnable = RunnableLambda(is_superuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8155a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(runnable.invoke({\"username\": \"admin\", \"password\": \"admin\"})) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91744f63",
   "metadata": {},
   "source": [
    "### Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c94ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../app.py\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langserve import add_routes\n",
    "\n",
    "from typing import Dict, Literal\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Llama Guard Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A Llama Guard Runnable built using Langchain's Runnable interfaces served on LangSmith\",\n",
    ")\n",
    "\n",
    "# Set all CORS enabled origins\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    "    expose_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Define runnable\n",
    "llm = OllamaLLM(model=\"llama-guard3\", temperature=0)\n",
    "parser = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "guard_chain = prompt | llm | parser\n",
    "\n",
    "# Second runnable\n",
    "def is_superuser(\n",
    "    credentials: Dict[\n",
    "        Literal[\"username\", \"password\"], str\n",
    "    ]\n",
    ") -> bool:\n",
    "    username = credentials.get(\"username\", None)\n",
    "    password = credentials.get(\"password\", None)\n",
    "    if username == \"admin\" and password == \"admin\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "runnable = RunnableLambda(is_superuser)\n",
    "\n",
    "# Final runnable\n",
    "map_chain = RunnableParallel(\n",
    "    guard_chain = guard_chain,\n",
    "    superuser_chain = runnable,\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    map_chain,\n",
    "    path=\"/gatekeeper\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45a0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guard_chain': 'safe', 'superuser_chain': False}\n"
     ]
    }
   ],
   "source": [
    "parallel_runnable = RemoteRunnable(\"http://localhost:8000/gatekeeper\")\n",
    "\n",
    "response = parallel_runnable.invoke(\n",
    "    {\n",
    "        \"input\": \"Why is the sky blue?\", \n",
    "        \"root\": {\n",
    "            \"username\": \"admin\",\n",
    "            \"password\": \"admin\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
